{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c61cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.activate(@__DIR__)\n",
    "Pkg.instantiate()\n",
    "using LinearAlgebra, Plots\n",
    "import ForwardDiff as FD\n",
    "using MeshCat\n",
    "using Test\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6665c60",
   "metadata": {},
   "source": [
    "# Q2: Equality Constrained Optimization (25 pts)\n",
    "In this problem, we are going to use Newton's method to solve some constrained optimization problems. We will start with a smaller problem where we can experiment with Full Newton vs Gauss-Newton, then we will use these methods to solve for the motor torques that make a quadruped balance on one leg."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0500ed",
   "metadata": {},
   "source": [
    "## Part A (10 pts)\n",
    "Here we are going to solve some equality-constrained optimization problems with Newton's method. We are given a problem \n",
    "\n",
    "$$ \\begin{align} \\min_x \\quad & f(x) \\\\ \n",
    " \\mbox{st} \\quad & c(x) = 0 \n",
    " \\end{align}$$\n",
    " Which has the following Lagrangian:\n",
    " $$ \\mathcal{L}(x,\\lambda) = f(x) + \\lambda ^T c(x), $$\n",
    "and the following KKT conditions for optimality:\n",
    "$$\\begin{align}\n",
    "\\nabla_x \\mathcal{L} = \\nabla_x f(x) + \\bigg[ \\frac{\\partial c}{\\partial x}\\bigg] ^T \\lambda &= 0 \\\\ \n",
    "c(x) &= 0 \n",
    "\\end{align}$$\n",
    "\n",
    "Which is just a root-finding problem. To solve this, we are going to solve for a $z = [x^T,\\lambda]^T$ that satisfies these KKT conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f089da1",
   "metadata": {},
   "source": [
    "### Newton's Method with a Linesearch\n",
    "\n",
    "We use Newton's method to solve for when $r(z) = 0$. To do this, we specify `res_fx(z)` as $r(z)$, and `res_jac_fx(z)` as $\\partial r/ \\partial z$. To calculate a Newton step, we do the following:\n",
    "\n",
    "$$\\Delta z = -\\bigg[ \\frac{\\partial r}{\\partial z} \\bigg]^{-1} r(z_k)$$\n",
    "\n",
    "We then decide the step length with a linesearch that finds the largest $\\alpha \\leq 1$ such that the following is true:\n",
    "$$ \\phi(z_k + \\alpha \\Delta z) < \\phi(z_k)$$\n",
    "Where $\\phi$ is a \"merit function\", or `merit_fx(z)` in the code. In this assignment you will use a backtracking linesearch where $\\alpha$ is initialized as $\\alpha = 1.0$, and is divided by 2 until the above condition is satisfied.\n",
    "\n",
    "NOTE: YOU DO NOT NEED TO (AND SHOULD NOT) USE A WHILE LOOP ANYWHERE IN THIS ASSIGNMENT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef62a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "function linesearch(z::Vector, Δz::Vector, merit_fx::Function;\n",
    "                    max_ls_iters = 10)::Float64 # optional argument with a default\n",
    "    \n",
    "    # TODO: return maximum α≤1 such that merit_fx(z + α*Δz) < merit_fx(z)\n",
    "    # with a backtracking linesearch (α = α/2 after each iteration)\n",
    "\n",
    "    # NOTE: DO NOT USE A WHILE LOOP \n",
    "    for i = 1:max_ls_iters\n",
    "\n",
    "        # TODO: return α when merit_fx(z + α*Δz) < merit_fx(z)\n",
    "        \n",
    "    end\n",
    "    error(\"linesearch failed\")\n",
    "end\n",
    "\n",
    "function newtons_method(z0::Vector, res_fx::Function, res_jac_fx::Function, merit_fx::Function;\n",
    "                        tol = 1e-10, max_iters = 50, verbose = false)::Vector{Vector{Float64}}\n",
    "    \n",
    "    # TODO: implement Newton's method given the following inputs:\n",
    "    # - z0, initial guess \n",
    "    # - res_fx, residual function \n",
    "    # - res_jac_fx, Jacobian of residual function wrt z \n",
    "    # - merit_fx, merit function for use in linesearch \n",
    "    \n",
    "    # optional arguments \n",
    "    # - tol, tolerance for convergence. Return when norm(residual)<tol \n",
    "    # - max iter, max # of iterations \n",
    "    # - verbose, bool telling the function to output information at each iteration\n",
    "    \n",
    "    # return a vector of vectors containing the iterates \n",
    "    # the last vector in this vector of vectors should be the approx. solution \n",
    "    \n",
    "    # NOTE: DO NOT USE A WHILE LOOP ANYWHERE \n",
    "    \n",
    "    # return the history of guesses as a vector\n",
    "    Z = [zeros(length(z0)) for i = 1:max_iters]\n",
    "    Z[1] = z0 \n",
    "    \n",
    "    \n",
    "    for i = 1:(max_iters - 1)\n",
    "        \n",
    "        # NOTE: everything here is a suggestion, do whatever you want to \n",
    "        \n",
    "        # TODO: evaluate current residual \n",
    "  \n",
    "        norm_r = 0.0 # TODO: update this \n",
    "        if verbose \n",
    "            print(\"iter: $i    |r|: $norm_r   \")\n",
    "        end\n",
    "        \n",
    "        # TODO: check convergence with norm of residual < tol \n",
    "        # if converged, return Z[1:i]\n",
    "        if norm_r < tol\n",
    "            return Z[1:i]\n",
    "        end\n",
    "        \n",
    "        # TODO: caculate Newton step (don't forget the negative sign)\n",
    "\n",
    "        \n",
    "        # TODO: linesearch and update z \n",
    "\n",
    "        if verbose\n",
    "            print(\"α: $α \\n\")\n",
    "        end\n",
    "        \n",
    "    end\n",
    "    error(\"Newton's method did not converge\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879bc600",
   "metadata": {},
   "outputs": [],
   "source": [
    "@testset \"check Newton\" begin \n",
    "    \n",
    "    f(_x) = [sin(_x[1]), cos(_x[2])]\n",
    "    df(_x) = FD.jacobian(f, _x)\n",
    "    merit(_x) = norm(f(_x))\n",
    "    \n",
    "    x0 = [-1.742410372590328, 1.4020334125022704]\n",
    "    \n",
    "    X = newtons_method(x0, f, df, merit; tol = 1e-10, max_iters = 50, verbose = true)\n",
    "    \n",
    "    # check this took the correct number of iterations\n",
    "    # if your linesearch isn't working, this will fail \n",
    "    # you should see 1 iteration where α = 0.5 \n",
    "    @test length(X) == 6 \n",
    "    \n",
    "    # check we actually converged\n",
    "    @test norm(f(X[end])) < 1e-10\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24da347c",
   "metadata": {},
   "source": [
    "We will now use Newton's method to solve the following constrained optimization problem. We will write functions for the full Newton Jacobian, as well as the Gauss-Newton Jacobian. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e6df2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    Q = [1.65539  2.89376; 2.89376  6.51521];\n",
    "    q = [2;-3]\n",
    "    cost(x) = 0.5*x'*Q*x + q'*x + exp(-1.3*x[1] + 0.3*x[2]^2) # cost function \n",
    "    contour(-1:.1:1,-1:.1:1, (x1,x2)-> cost([x1;x2]),title = \"Cost Function\",\n",
    "            xlabel = \"X₁\", ylabel = \"X₂\",fill = true)\n",
    "        plot!(-1:.1:1, -0.3*(-1:.1:1).^2 - 0.3*(-1:.1:1) .- .2,lw = 3,label = \"constraint\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a086487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use Newton's method to solve the constrained optimization problem shown above\n",
    "function cost(x::Vector)\n",
    "    Q = [1.65539  2.89376; 2.89376  6.51521];\n",
    "    q = [2;-3]\n",
    "    return 0.5*x'*Q*x + q'*x + exp(-1.3*x[1] + 0.3*x[2]^2)\n",
    "end\n",
    "function constraint(x::Vector)\n",
    "    norm(x) - 0.5 \n",
    "end\n",
    "# HINT: use this if you want to, but you don't have to\n",
    "function constraint_jacobian(x::Vector)::Matrix\n",
    "    # since `constraint` returns a scalar value, ForwardDiff \n",
    "    # will only allow us to compute a gradient of this function \n",
    "    # (instead of a Jacobian). This means we have two options for\n",
    "    # computing the Jacobian: Option 1 is to just reshape the gradient\n",
    "    # into a row vector\n",
    "    \n",
    "    # J = reshape(FD.gradient(constraint, x), 1, 2)\n",
    "    \n",
    "    # or we can just make the output of constraint an array, \n",
    "    constraint_array(_x) = [constraint(_x)]\n",
    "    J = FD.jacobian(constraint_array, x)\n",
    "    \n",
    "    # assert the jacobian has # rows = # outputs \n",
    "    # and # columns = # inputs \n",
    "    @assert size(J) == (length(constraint(x)), length(x))\n",
    "    \n",
    "    return J \n",
    "end\n",
    "function kkt_conditions(z::Vector)::Vector\n",
    "    # TODO: return the KKT conditions\n",
    "\n",
    "    x = z[1:2]\n",
    "    λ = z[3:3]\n",
    "\n",
    "    # TODO: return the stationarity condition for the cost function\n",
    "    # and the primal feasibility\n",
    "\n",
    "    error(\"kkt not implemented\")\n",
    "    return 0*z \n",
    "end\n",
    "\n",
    "function fn_kkt_jac(z::Vector)::Matrix\n",
    "    # TODO: return full Newton Jacobian of kkt conditions wrt z\n",
    "    x = z[1:2]\n",
    "    λ = z[3]\n",
    "\n",
    "    # TODO: return full Newton jacobian with a 1e-3 regularizer\n",
    "    error(\"fn_kkt_jac not implemented\")\n",
    "    return nothing \n",
    "end\n",
    "function gn_kkt_jac(z::Vector)::Matrix\n",
    "    # TODO: return Gauss-Newton Jacobian of kkt conditions wrt z \n",
    "    x = z[1:2]\n",
    "    λ = z[3]\n",
    "\n",
    "    # TODO: return Gauss-Newton jacobian with a 1e-3 regularizer\n",
    "    error(\"gn_kkt_jac not implemented\")\n",
    "    return nothing \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9fb793",
   "metadata": {},
   "outputs": [],
   "source": [
    "@testset \"Test Jacobians\" begin \n",
    "    \n",
    "    # first we check the regularizer \n",
    "    z = randn(3)\n",
    "    J_fn = fn_kkt_jac(z)\n",
    "    J_gn = gn_kkt_jac(z)\n",
    "    \n",
    "    # check what should/shouldn't be the same between \n",
    "    @test norm(J_fn[1:2,1:2] - J_gn[1:2,1:2]) > 1e-10\n",
    "    @test abs(J_fn[3,3] + 1e-3) < 1e-10\n",
    "    @test abs(J_gn[3,3] + 1e-3) < 1e-10\n",
    "    @test norm(J_fn[1:2,3] - J_gn[1:2,3]) < 1e-10\n",
    "    @test norm(J_fn[3,1:2] - J_gn[3,1:2]) < 1e-10\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46b09aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@testset \"Full Newton\" begin \n",
    "    \n",
    "    \n",
    "    z0 = [-.1, .5, 0] # initial guess\n",
    "    merit_fx(_z) = norm(kkt_conditions(_z)) # simple merit function\n",
    "    Z = newtons_method(z0, kkt_conditions, fn_kkt_jac, merit_fx; tol = 1e-4, max_iters = 100, verbose = true)\n",
    "    R = kkt_conditions.(Z)\n",
    "\n",
    "    # make sure we converged on a solution to the KKT conditions \n",
    "    @test norm(kkt_conditions(Z[end])) < 1e-4\n",
    "    @test length(R) < 6\n",
    "    \n",
    "    \n",
    "    # ------------------------plotting stuff------------------------\n",
    "    Rp = [[abs(R[i][ii]) + 1e-15 for i = 1:length(R)] for ii = 1:length(R[1])] # this gets abs of each term at each iteration\n",
    "    \n",
    "    plot(Rp[1],yaxis=:log,ylabel = \"|r|\",xlabel = \"iteration\",\n",
    "         yticks= [1.0*10.0^(-x) for x = float(15:-1:-2)],\n",
    "         title = \"Convergence of Full Newton on KKT Conditions\",label = \"|r_1|\")\n",
    "    plot!(Rp[2],label = \"|r_2|\")\n",
    "    display(plot!(Rp[3],label = \"|r_3|\"))\n",
    "    \n",
    "    contour(-.6:.1:0,0:.1:.6, (x1,x2)-> cost([x1;x2]),title = \"Cost Function\",\n",
    "            xlabel = \"X₁\", ylabel = \"X₂\",fill = true)\n",
    "    xcirc = [.5*cos(θ) for θ in range(0, 2*pi, length = 200)]\n",
    "    ycirc = [.5*sin(θ) for θ in range(0, 2*pi, length = 200)]\n",
    "    plot!(xcirc,ycirc, lw = 3.0, xlim = (-.6, 0), ylim = (0, .6),label = \"constraint\")\n",
    "    z1_hist = [z[1] for z in Z]\n",
    "    z2_hist = [z[2] for z in Z]\n",
    "    display(plot!(z1_hist, z2_hist, marker = :d, label = \"xₖ\"))\n",
    "    # ------------------------plotting stuff------------------------\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b1f812",
   "metadata": {},
   "outputs": [],
   "source": [
    "@testset \"Gauss-Newton\" begin \n",
    "    \n",
    "    \n",
    "    z0 = [-.1, .5, 0] # initial guess\n",
    "    merit_fx(_z) = norm(kkt_conditions(_z)) # simple merit function\n",
    "    \n",
    "    # the only difference in this block vs the previous is `gn_kkt_jac` instead of `fn_kkt_jac`\n",
    "    Z = newtons_method(z0, kkt_conditions, gn_kkt_jac, merit_fx; tol = 1e-4, max_iters = 100, verbose = true)\n",
    "    R = kkt_conditions.(Z)\n",
    "\n",
    "    # make sure we converged on a solution to the KKT conditions \n",
    "    @test norm(kkt_conditions(Z[end])) < 1e-4\n",
    "    @test length(R) < 10\n",
    "    \n",
    "    \n",
    "    # ------------------------plotting stuff------------------------\n",
    "    Rp = [[abs(R[i][ii]) + 1e-15 for i = 1:length(R)] for ii = 1:length(R[1])] # this gets abs of each term at each iteration\n",
    "    \n",
    "    plot(Rp[1],yaxis=:log,ylabel = \"|r|\",xlabel = \"iteration\",\n",
    "         yticks= [1.0*10.0^(-x) for x = float(15:-1:-2)],\n",
    "         title = \"Convergence of Full Newton on KKT Conditions\",label = \"|r_1|\")\n",
    "    plot!(Rp[2],label = \"|r_2|\")\n",
    "    display(plot!(Rp[3],label = \"|r_3|\"))\n",
    "    \n",
    "    contour(-.6:.1:0,0:.1:.6, (x1,x2)-> cost([x1;x2]),title = \"Cost Function\",\n",
    "            xlabel = \"X₁\", ylabel = \"X₂\",fill = true)\n",
    "    xcirc = [.5*cos(θ) for θ in range(0, 2*pi, length = 200)]\n",
    "    ycirc = [.5*sin(θ) for θ in range(0, 2*pi, length = 200)]\n",
    "    plot!(xcirc,ycirc, lw = 3.0, xlim = (-.6, 0), ylim = (0, .6),label = \"constraint\")\n",
    "    z1_hist = [z[1] for z in Z]\n",
    "    z2_hist = [z[2] for z in Z]\n",
    "    display(plot!(z1_hist, z2_hist, marker = :d, label = \"xₖ\"))\n",
    "    # ------------------------plotting stuff------------------------\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2e5747",
   "metadata": {},
   "source": [
    "## Part B (10 pts): Balance a quadruped\n",
    "Now we are going to solve for the control input $u \\in \\mathbb{R}^{12}$, and state $x \\in \\mathbb{R}^{30}$, such that the quadruped is balancing up on one leg. First, let's load in a model and display the rough \"guess\" configuration that we are going for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50ea3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(joinpath(@__DIR__, \"quadruped.jl\"))\n",
    "\n",
    "# --------these three are global variables------------\n",
    "model = UnitreeA1()\n",
    "mvis = initialize_visualizer(model)\n",
    "const x_guess = initial_state(model)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "set_configuration!(mvis, x_guess[1:state_dim(model)÷2])\n",
    "render(mvis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5826be3",
   "metadata": {},
   "source": [
    "Now, we are going to solve for the state and control that get us a statically stable stance on just one leg. We are going to do this by solving the following optimization problem:\n",
    "\n",
    "$$ \\begin{align} \\min_{x,u} \\quad & \\frac{1}{2}(x - x_{guess})^T(x - x_{guess}) + \\frac{1}{2}10^{-3} u^Tu \\\\ \n",
    " \\mbox{st} \\quad & f(x,u) = 0 \n",
    " \\end{align}$$\n",
    " \n",
    " Where our primal variables are $x \\in \\mathbb{R}^{30}$ and $u \\in \\mathbb{R}^{12}$, that we can stack up in a new variable $y = [x^T, u^T]^T \\in \\mathbb{R}^{42}$. We have a constraint $f(x,u) = \\dot{x} = 0$, which will ensure the resulting configuration is stable. This constraint is enforced with a dual variable $\\lambda \\in \\mathbb{R}^{30}$. We are now ready to use Newton's method to solve this equality constrained optimization problem, where we will solve for a variable $z = [y^T, \\lambda^T]^T \\in \\mathbb{R}^{72}$.\n",
    " \n",
    " In this next section, you should fill out `quadruped_kkt(z)` with the KKT conditions for this optimization problem, given the constraint is that `dynamics(model, x, u) = zeros(30)`. When forming the Jacobian of the KKT conditions, use the Gauss-Newton approximation for the hessian of the Lagrangian (see example above if you're having trouble with this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6bfbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial guess \n",
    "const x_guess = initial_state(model)\n",
    "\n",
    "# indexing stuff \n",
    "const idx_x = 1:30 \n",
    "const idx_u = 31:42\n",
    "const idx_c = 43:72\n",
    "\n",
    "# I like stacking up all the primal variables in y, where y = [x;u]\n",
    "# Newton's method will solve for z = [x;u;λ], or z = [y;λ]\n",
    "\n",
    "function quadruped_cost(y::Vector)\n",
    "    # cost function \n",
    "    @assert length(y) == 42\n",
    "    x = y[idx_x]\n",
    "    u = y[idx_u]\n",
    "    \n",
    "    # TODO: return cost \n",
    "    error(\"quadruped cost not implemented\")\n",
    "    return 0.0\n",
    "end\n",
    "function quadruped_constraint(y::Vector)::Vector\n",
    "    # constraint function \n",
    "    @assert length(y) == 42\n",
    "    x = y[idx_x]\n",
    "    u = y[idx_u]\n",
    "    \n",
    "    # TODO: return constraint\n",
    "    error(\"quadruped constraint not implemented\")\n",
    "    return 0*y\n",
    "end\n",
    "function quadruped_kkt(z::Vector)::Vector\n",
    "    @assert length(z) == 72 \n",
    "    x = z[idx_x]\n",
    "    u = z[idx_u]\n",
    "    λ = z[idx_c]\n",
    "    \n",
    "    y = [x;u]\n",
    "    \n",
    "    # TODO: return the KKT conditions \n",
    "    error(\"quadruped kkt not implemented\")\n",
    "    return 0*z\n",
    "end\n",
    "\n",
    "function quadruped_kkt_jac(z::Vector)::Matrix\n",
    "    @assert length(z) == 72 \n",
    "    x = z[idx_x]\n",
    "    u = z[idx_u]\n",
    "    λ = z[idx_c]\n",
    "    \n",
    "    y = [x;u]\n",
    "    \n",
    "    # TODO: return Gauss-Newton Jacobian with a regularizer (try 1e-3,1e-4,1e-5,1e-6)\n",
    "    # and use whatever regularizer works for you\n",
    "    error(\"quadruped kkt jac not implemented\")\n",
    "    return zeros(length(z), length(z))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa84cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "function quadruped_merit(z)\n",
    "    # merit function for the quadruped problem \n",
    "    @assert length(z) == 72 \n",
    "    r = quadruped_kkt(z)\n",
    "    return norm(r[1:42]) + 1e4*norm(r[43:end])\n",
    "end\n",
    "\n",
    "@testset \"quadruped standing\" begin\n",
    "    \n",
    "    z0 = [x_guess; zeros(12); zeros(30)]\n",
    "    Z = newtons_method(z0, quadruped_kkt, quadruped_kkt_jac, quadruped_merit; tol = 1e-6, verbose = true, max_iters = 50)\n",
    "    set_configuration!(mvis, Z[end][1:state_dim(model)÷2])\n",
    "    R = norm.(quadruped_kkt.(Z))\n",
    "    \n",
    "    display(plot(1:length(R), R, yaxis=:log,xlabel = \"iteration\", ylabel = \"|r|\"))\n",
    "    \n",
    "    @test R[end] < 1e-6\n",
    "    @test length(Z) < 25\n",
    "    \n",
    "    x,u = Z[end][idx_x], Z[end][idx_u]\n",
    "    \n",
    "    @test norm(dynamics(model, x, u)) < 1e-6\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a570f0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    \n",
    "    # let's visualize the balancing position we found\n",
    "    \n",
    "    z0 = [x_guess; zeros(12); zeros(30)]\n",
    "    Z = newtons_method(z0, quadruped_kkt, quadruped_kkt_jac, quadruped_merit; tol = 1e-6, verbose = false, max_iters = 50)\n",
    "    # visualizer \n",
    "    mvis = initialize_visualizer(model)\n",
    "    set_configuration!(mvis, Z[end][1:state_dim(model)÷2])\n",
    "    render(mvis)\n",
    "    \n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bc85af",
   "metadata": {},
   "source": [
    "## Part C (5 pts): One sentence short answer\n",
    "\n",
    "1. Why do we use a linesearch? \n",
    "\n",
    "**put ONE SENTENCE answer here**\n",
    "\n",
    "2. Do we need a linesearch for both convex and nonconvex problems?\n",
    "\n",
    "**put ONE SENTENCE answer here**\n",
    "\n",
    "3. Name one case where we absolutely do not need a linesearch.\n",
    "\n",
    "**put ONE SENTENCE answer here**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.2",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
